# Chenfei WU (å´æ™¨é£)

[Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=1YlFL5UAAAAJ) \| [Github](https://github.com/chenfei-wu) \| [LinkedIn](https://www.linkedin.com/in/chenfei-wu-544986167/) \|[Twitter](https://x.com/wu_chenfei)\| [fulai.hr@alibaba-inc.com (Work)](fulai.hr@alibaba-inc.com) \| [cqwuchenfei@163.com (Personal)](cqwuchenfei@163.com)

Chenfei Wu received his Ph.D. from Beijing University of Posts and Telecommunications, and is currently a senior expert at Tongyi Lab, Alibaba. His research focuses on large model pretraining, multimodal understanding, and generation. His main research includes a series of multimodal generation models NUWA (NUWA, NUWA-LIP, NUWA-Infinity, NUWA-3D, NUWA-XL), Step-Video (Step-Video-T2V, Step-Video-TI2V) and Qwen-Image, a series of multimodal understanding models (KD-VLP, Bridge-Tower), and multimodal dialogue systems (Visual ChatGPT, TaskMatrix.AI). He published several papers in conferences such as CVPR, NeurIPS, ACL, ECCV, AAAI, MM, with more than 5000 citations. His Github open source projects have been liked more than 30,000 times.

å´æ™¨é£ï¼ŒåŒ—äº¬é‚®ç”µå¤§å­¦åšå£«ï¼Œé˜¿é‡Œå·´å·´é€šä¹‰å®éªŒå®¤èµ„æ·±ä¸“å®¶ã€‚ç ”ç©¶æ–¹å‘ä¸ºå¤§æ¨¡å‹é¢„è®­ç»ƒã€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ä¸»è¦ç ”ç©¶å·¥ä½œåŒ…æ‹¬å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ NUWAï¼ˆå¥³å¨²ï¼‰ç³»åˆ—ï¼ˆNUWA, NUWA-Infinity, NUWA-XL, DragNUWAï¼‰, Step-Videoç³»åˆ—ï¼ˆStep-Video-T2V, Step-Video-TI2Vï¼‰, Qwenç”Ÿæˆç³»åˆ—(Qwen-Image)ã€å¤šæ¨¡æ€ç†è§£æ¨¡å‹ Bridge Towerï¼ˆæ¡¥å¡”ï¼‰ç³»åˆ—ï¼ˆKD-VLP, Bridge-Towerï¼‰ä»¥åŠå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿï¼ˆVisual ChatGPT, TaskMatrix.AIï¼‰ã€‚åœ¨ CVPR, NeurIPS, ACL, ECCV, AAAI, MM ç­‰ä¼šå‘è¡¨å¤šç¯‡è®ºæ–‡ï¼Œå¼•ç”¨é‡5000ä½™æ¬¡, Github å¼€æºé¡¹ç›®è·èµä¸‰ä¸‡ä½™æ¬¡ã€‚

## News
We release a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source!

### ğŸ” Key Highlights

- **SOTA text rendering** â€” rivals GPT-4o in English, best-in-class for Chinese  
- **In-pixel text generation** â€” no overlays, fully integrated  
- **Bilingual support**, diverse fonts, complex layouts  

### ğŸ“š Resources

- **Blog**: https://qwenlm.github.io/blog/qwen-image/  
- **Hugging Face**: https://huggingface.co/Qwen/Qwen-Image  
- **ModelScope**: https://modelscope.cn/models/Qwen/Qwen-Image  
- **GitHub**: https://github.com/QwenLM/Qwen-Image  
- **Technical Report**: https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf  
- **Demo**: https://modelscope.cn/aigc/imageGeneration?tab=advanced

## Highlight

- **Multimodal Generation**: [GODIVA](https://arxiv.org/abs/2104.14806) (Preprint, 2021), [NUWA(å¥³å¨²)](https://arxiv.org/abs/2111.12417) (ECCV, 2022, [![](https://img.shields.io/github/stars/microsoft/NUWA?style=social&label=Github+Stars)](https://github.com/microsoft/NUWA.git)), [NUWA-Infinity](https://arxiv.org/abs/2207.09814) (NeurIPS, 2022), [NUWA-LIP](https://arxiv.org/abs/2202.05009) (CVPR 2023), [NUWA-3D](https://arxiv.org/abs/2302.10781) (IJCAI 2023), [NUWA-XL](https://arxiv.org/abs/2303.12346) (ACL 2023), [DragNUWA](https://arxiv.org/abs/2308.08089) (Preprint, 2023), [LayoutNUWA](https://openreview.net/forum?id=qCUWVT0Ayy) (ICLR 2024), [StrokeNUWA](https://arxiv.org/abs/2401.17093) (Preprint, 2024), [Step-Video-T2V](https://arxiv.org/abs/2502.10248) (Preprint, 2025), [Step-Video-TI2V](https://arxiv.org/abs/2503.11251) (Preprint, 2025), [Qwen-Image](https://arxiv.org/abs/2508.02324) (Preprint, 2025,[![](https://img.shields.io/github/stars/QwenLM/Qwen-Image?style=social&label=Github+Stars)](https://github.com/QwenLM/Qwen-Image)).
- **Multimodal Understanding**: [Bridge-Tower](https://arxiv.org/abs/2206.08657) (AAAI, 2023), [Manager-Tower](https://arxiv.org/abs/2306.00103) (ACL, 2023)
- **Multimodal System**: [Visual ChatGPT](https://arxiv.org/abs/2303.04671) (Preprint, 2023, [![](https://img.shields.io/github/stars/chenfei-wu/TaskMatrix?style=social&label=Github+Stars)](https://github.com/chenfei-wu/TaskMatrix)), [TaskMatrix.AI](https://arxiv.org/abs/2303.16434) (Intelligent Computing, 2024) [VL-InterpreT](https://openaccess.thecvf.com/content/CVPR2022/papers/Aflalo_VL-InterpreT_An_Interactive_Visualization_Tool_for_Interpreting_Vision-Language_Transformers_CVPR_2022_paper.pdf) (CVPR, 2022).

## Talks

- [NUWA: Neural visual world creation with multimodal pretraining](https://www.microsoft.com/en-us/research/video/research-talk-nuwa-neural-visual-world-creation-with-multimodal-pretraining/). Microsoft Research Summit 2021, October 2021.
- [VLP for Text-to-Image Synthesis](https://www.microsoft.com/en-us/research/video/vlp-tutorial-cvpr-2022-vlp-for-text-to-image-synthesis/). VLP Tutorial @ CVPR 2022, Jun 2022.
- [å¼€æ”¾æŠ¥åï½œé¡¶å°–ä¸“å®¶è”åˆæ‰“é€ ï¼Œé¦–ä¸ªç³»ç»ŸåŒ– AI å¤§æ¨¡å‹å‰æ²¿æŠ€æœ¯è®²ä¹ ç­](https://mp.weixin.qq.com/s/z4cHRj0wv8CeJGEhXkFwNw). æ™ºæºç¤¾åŒº, March 2023.
- [æ˜Ÿè¾°å¤§æµ· äºˆåŠ›åŒè¡Œ é¨æ¸¸â€œAIGC+å…ƒå®‡å®™â€ä¸–ç•Œï¼ŒæŒè¡Œä¸šé£å£ï¼Œå èµ›é“å…ˆæœº](https://mp.weixin.qq.com/s/CdgOuNL5dGXAx0xHizEvGw). å¾®è½¯ç§‘æŠ€, March 2023.
- [ä¸­å›½ä¸­æ–‡ä¿¡æ¯å­¦ä¼šã€Šå‰æ²¿æŠ€æœ¯è®²ä¹ ç­ã€‹- å¤§æ¨¡å‹ç³»åˆ—ä¸“é¢˜ Â· æ·±åœ³ç«™](https://mp.weixin.qq.com/s/lv8fKBf4jUq5hMpr_Vk1bw). ä¸­å›½ä¸­æ–‡ä¿¡æ¯å­¦ä¼š, Jun 2023.
- [A2M å³°ä¼šåœ†æ»¡è½å¹• AIGC æ—¶ä»£ä¸‹çš„ AI è½åœ°å®è·µã€æ•°æ®æ™ºèƒ½å’ŒåŸºç¡€æ¶æ„æ¼”è¿›](https://mp.weixin.qq.com/s/0-NuH5qDe5afFmiBtdzgPQ). msup, Jun 2023.
- [MLNLP2023@å¤šæ¨¡æ€å¤šè¯­è¨€å¤§æ¨¡å‹è®ºå›](https://mp.weixin.qq.com/s/vB2zn0CTwO_iyda0OojbVw). MLNLP, Sep 2023.
- [ä¸­å›½ä¸­æ–‡ä¿¡æ¯å­¦ä¼šã€Šå‰æ²¿æŠ€æœ¯è®²ä¹ ç­ã€‹-å¤§æ¨¡å‹ç³»åˆ—ä¸“é¢˜Â·æˆéƒ½ç«™](https://mp.weixin.qq.com/s/F6CPgrGWSDUqMm2Nfg3ijQ). ä¸­å›½ä¸­æ–‡ä¿¡æ¯å­¦ä¼š, Nov 2023.

## Media Report

- [å¾®è½¯å†æ‰” AI èŠå¤©ç”»å›¾ç‚¸å¼¹ï¼è§†è§‰æ¨¡å‹åŠ æŒ ChatGPTï¼ŒVisual ChatGPT æ¨ªç©ºå‡ºä¸–](https://mp.weixin.qq.com/s/Xg-MRtqBt6ONKnLJYFw0Ww). æ–°æ™ºå…ƒ, March 2023.
- [è§†è§‰ç‰ˆ ChatGPT æ¥äº†ï¼å¸æ”¶ AI ç”»ç”»å…¨æŠ€èƒ½ï¼ŒMSRA å…¨åäººå›¢é˜Ÿæ‰“é€ ï¼Œå¾®è½¯ 16 å¹´è€å°†é¢†è¡”](https://mp.weixin.qq.com/s/oanSkopLM93Krx2jVozR_A). é‡å­ä½, March 2023.
- [ä¸€ä¸ª AI é©±åŠ¨ç™¾ä¸‡ä¸ª APIï¼å¾®è½¯æå‡ºå¤šä»»åŠ¡å¤„ç†æ¨¡å‹ TaskMatrixï¼Œæœºå™¨äººå’Œç‰©è”ç½‘ç»ˆäºæœ‰æ•‘äº†](https://mp.weixin.qq.com/s/_mDyCiqSqlWi4zdtrfxOKw). é‡å­ä½, March 2023.
- [å¾®è½¯äºšæ´²ç ”ç©¶é™¢å¤šæ¨¡æ€æ¨¡å‹ NÃœWAï¼šä»¥è‡ªç„¶è¯­è¨€åˆ›é€ è§†è§‰å†…å®¹](https://mp.weixin.qq.com/s/oyBSoGxJxMGuesO5ea4TxQ). å¾®è½¯äºšæ´²ç ”ç©¶é™¢, March 2022.
- [åƒä¸‡åˆ«è®©å¯Œåšä¹‰åšçœ‹åˆ°è¿™ä¸ª-NUWA-Infinity](https://mp.weixin.qq.com/s/xiY0gGwN1V-uxUEIAFgJ7g). é‡å­ä½, Jul 2022.
- [NUWA ç³»åˆ—å†æ·»æ–°æˆå‘˜â€”â€”è¶…é•¿è§†é¢‘ç”Ÿæˆæ¨¡å‹ NUWA-XL](https://mp.weixin.qq.com/s/1Ewuvtx4Hudu6s07GQ2Vyg). å¾®è½¯äºšæ´²ç ”ç©¶é™¢, Feb 2023.
- [å¸¦ä½ ç©¿è¶Šæ¸…æ˜ä¸Šæ²³å›¾ï¼DragNUWA æƒŠè‰³äº®ç›¸ï¼šä¸€æ‹–ä¸€æ‹½è®©é™å›¾ç§’å˜è§†é¢‘](https://mp.weixin.qq.com/s/sgT5x-6rkFLqs4AEmBqyCw). æ–°æ™ºå…ƒ, Sep 2023.

## Publications

### Multimodal Generation

- **Godiva: Generating open-domain videos from natural descriptions**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2104.14806)
  <br> **Chenfei Wu**, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, Nan Duan.
  <br> Arxiv, 2021

- **NÃ¼wa: Visual synthesis pre-training for neural visual world creation**. [![](https://img.shields.io/badge/Paper-378CE7)](https://link.springer.com/chapter/10.1007/978-3-031-19787-1_41) [![](https://img.shields.io/github/stars/microsoft/NUWA?style=social&label=Github+Stars)](https://github.com/microsoft/NUWA.git)
  <br> **Chenfei Wu**, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan.
  <br> ECCV, 2022.

- **NUWA-LIP: language-guided image inpainting with defect-free VQGAN**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2202.05009)
  <br> Minheng Ni, **Chenfei Wu**, Haoyang Huang, Daxin Jiang, Wangmeng Zuo, Nan Duan.
  <br> CVPR 2023.

- **NUWA-Infinity: Autoregressive over autoregressive generation for infinite visual synthesis**. [![](https://img.shields.io/badge/Paper-378CE7)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/6358cd0cd6607fdf4870595795eb1710-Abstract-Conference.html) [![](https://img.shields.io/badge/Homepage-FF8E8F)](https://nuwa-infinity.microsoft.com/#/NUWAInfinity) [![](https://img.shields.io/github/stars/microsoft/NUWA?style=social&label=Github+Stars)](https://github.com/microsoft/NUWA.git)
  <br> Jian Liang, **Chenfei Wu**, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan.
  <br> CVPR 2022.

- **NUWA-XL: Diffusion over diffusion for extremely long video generation**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2303.12346) [![](https://img.shields.io/badge/Homepage-FF8E8F)](https://nuwa-infinity.microsoft.com/#/NUWAXL) [![](https://img.shields.io/github/stars/microsoft/NUWA?style=social&label=Github+Stars)](https://github.com/microsoft/NUWA.git)
  <br> Shengming Yin, **Chenfei Wu**, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, Nan Duan.
  <br> ACL 2023.

- **DragNUWA: Fine-grained control in video generation by integrating text, image, and trajectory**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2308.08089) [![](https://img.shields.io/badge/Homepage-FF8E8F)](https://nuwa-infinity.microsoft.com/#/) [![](https://img.shields.io/github/stars/ProjectNUWA/DragNUWA?style=social&label=Github+Stars)](https://github.com/ProjectNUWA/DragNUWA)
  <br> Shengming Yin, **Chenfei Wu**, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, Nan Duan.
  <br> Arxiv 2023.

- **StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2401.17093)
  <br> Zecheng Tang, **Chenfei Wu**, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu Liu, Zhengyuan Yang, Lijuan Wang, Zicheng Liu, Juntao Li, Nan Duan.
  <br> Arxiv 2024.

- **LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models**. [![](https://img.shields.io/badge/Paper-378CE7)](https://openreview.net/forum?id=qCUWVT0Ayy) [![](https://img.shields.io/github/stars/ProjectNUWA/LayoutNUWA?style=social&label=Github+Stars)](https://github.com/ProjectNUWA/LayoutNUWA)
  <br> Zecheng Tang, **Chenfei Wu**, Juntao Li, Nan Duan.
  <br> ICLR 2024.

- **NUWA-3D: Learning 3D photography videos via self-supervised diffusion on single images**. [![](https://img.shields.io/badge/Paper-378CE7)](https://dl.acm.org/doi/abs/10.24963/ijcai.2023/167)
  <br> Xiaodong Wang, **Chenfei Wu**, Shengming Yin, Minheng Ni, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Fan Yang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan.
  <br> IJCAI 2023.

- **HORIZON: A High-Resolution Panorama Synthesis Framework**. [![](https://img.shields.io/badge/Paper-378CE7)](https://dl.acm.org/doi/abs/10.24963/ijcai.2023/167)
  <br> Kun Yan, Lei Ji, **Chenfei Wu**, Jian Liang, Ming Zhou, Nan Duan, Shuai Ma.
  <br> AAAI 2024.

- **Trace Controlled Text to Image Generation**. [![](https://img.shields.io/badge/Paper-378CE7)](https://link.springer.com/chapter/10.1007/978-3-031-20059-5_4)
  <br> Kun Yan, Lei Ji, Chenfei Wu, Jianmin Bao, Ming Zhou, Nan Duan, Shuai Ma.
  <br> ECCV, 2022.

- **ORES: Open-vocabulary Responsible Visual Synthesis**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2308.13785)
  <br> Minheng Ni, **Chenfei Wu**, Xiaodong Wang, Shengming Yin, Lijuan Wang, Zicheng Liu, Nan Duan.
  <br> AAAI 2024.

- **Reco: Region-controlled text-to-image generation**. [![](https://img.shields.io/badge/Paper-378CE7)](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.html) [![](https://img.shields.io/github/stars/microsoft/ReCo?style=social&label=Github+Stars)](https://github.com/microsoft/ReCo)
  <br> Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, **Chenfei Wu**, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang.
  <br> CVPR, 2023.

- **DiVAE: Photorealistic images synthesis with denoising diffusion decoder**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2206.00386)
  <br> Jie Shi, **Chenfei Wu**, Jian Liang, Xiang Liu, Nan Duan.
  <br> Arxiv 2022.

### Multimodal Understanding

- **Using Left and Right Brains Together: Towards Vision and Language Planning**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2402.10534)
  <br> Jun Cen, Chenfei Wu, Xiao Liu, Shengming Yin, Yixuan Pei, Jinglong Yang, Qifeng Chen, Nan Duan, Jianguo Zhang.
  <br> Arxiv 2024.

- **Kd-vlp: Improving end-to-end vision-and-language pretraining with object knowledge distillation**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2109.10504)
  <br> Yongfei Liu, Chenfei Wu, Shao-yen Tseng, Vasudev Lal, Xuming He, Nan Duan.
  <br> Findings of NAACL, 2022.

- **Bridgetower: Building bridges between encoders in vision-language representation learning**. [![](https://img.shields.io/badge/Paper-378CE7)](https://ojs.aaai.org/index.php/AAAI/article/view/26263) [![](https://img.shields.io/github/stars/microsoft/BridgeTower?style=social&label=Github+Stars)](https://github.com/microsoft/BridgeTower)
  <br> Xiao Xu, **Chenfei Wu**, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.
  <br> AAAI 2023.

- **ManagerTower: Aggregating the insights of uni-modal experts for vision-language representation learning**. [![](https://img.shields.io/badge/Paper-378CE7)](https://aclanthology.org/2023.acl-long.811/)
  <br> Xiao Xu, Bei Li, **Chenfei Wu**, Shao-Yen Tseng, Anahita Bhiwandiwalla, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.
  <br> ACL 2023.

- **Learning temporal video procedure segmentation from an automatically collected large dataset**. [![](https://img.shields.io/badge/Paper-378CE7)](https://openaccess.thecvf.com/content/WACV2022/html/Ji_Learning_Temporal_Video_Procedure_Segmentation_From_an_Automatically_Collected_Large_WACV_2022_paper.html)
  <br> Lei Ji, **Chenfei Wu**, Daisy Zhou, Kun Yan, Edward Cui, Xilin Chen, Nan Duan.
  <br> WACV 2022.

- **Deep reason: A strong baseline for real-world visual reasoning**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/1905.10226)
  <br> **Chenfei Wu**, Yanzhao Zhou, Gen Li, Nan Duan, Duyu Tang, Xiaojie Wang.
  <br> CVPR VQA Workshop, 2019.
- **Object-difference attention: A simple relational attention for visual question answering**. [![](https://img.shields.io/badge/Paper-378CE7)](https://dl.acm.org/doi/abs/10.1145/3240508.3240513)
  <br> **Chenfei Wu**, Jinlai Liu, Xiaojie Wang, Xuan Dong
  <br> ACM Multimedia, 2018

- **Chain of reasoning for visual question answering**. [![](https://img.shields.io/badge/Paper-378CE7)](https://proceedings.neurips.cc/paper_files/paper/2018/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html)
  <br> **Chenfei Wu**, Jinlai Liu, Xiaojie Wang, Xuan Dong.
  <br> NeurIPS, 2018

- **Differential networks for visual question answering**. [![](https://img.shields.io/badge/Paper-378CE7)](https://ojs.aaai.org/index.php/AAAI/article/view/4930)
  <br>**Chenfei Wu**, Jinlai Liu, Xiaojie Wang, Ruifan Li.
  <br> AAAI, 2019.

- **Sequential visual reasoning for visual question answering**. [![](https://img.shields.io/badge/Paper-378CE7)](https://ieeexplore.ieee.org/abstract/document/8691361/)
  <br> Jinlai Liu, Chenfei Wu, Xiaojie Wang, Xuan Dong.
  <br> CCIS 2018.

### Multimodal Systems/Evaluations

- **Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2303.04671) [![](https://img.shields.io/github/stars/chenfei-wu/TaskMatrix?style=social&label=Github+Stars)](https://github.com/chenfei-wu/TaskMatrix)
  <br> **Chenfei Wu**, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan.
  <br> arXiv, 2023.

- **Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2303.16434) [![](https://img.shields.io/github/stars/chenfei-wu/TaskMatrix?style=social&label=Github+Stars)](https://github.com/chenfei-wu/TaskMatrix)
  <br> Yaobo Liang, **Chenfei Wu**, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, Nan Duan.
  <br> Intelligent Computing, 2024

- **Vl-interpret: An interactive visualization tool for interpreting vision-language transformers**. [![](https://img.shields.io/badge/Paper-378CE7)](https://openaccess.thecvf.com/content/CVPR2022/html/Aflalo_VL-InterpreT_An_Interactive_Visualization_Tool_for_Interpreting_Vision-Language_Transformers_CVPR_2022_paper.html)
  <br> Estelle Aflalo, Meng Du, Shao-Yen Tseng, Yongfei Liu, **Chenfei Wu**, Nan Duan, Vasudev Lal.
  <br> CVPR 2022.

- **Low-code llm: Visual programming over llms**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2304.08103)
  <br> Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge, **Chenfei Wu**, Wang You, Ting Song, Yan Xia, Jonathan Tien, Nan Duan.
  <br> Arxiv 2023.

- **Learning to program with natural language**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2304.10464)
  <br> Yiduo Guo, Yaobo Liang, **Chenfei Wu**, Wenshan Wu, Dongyan Zhao, Nan Duan.
  <br> Arxiv 2023.

- **GEM: A general evaluation benchmark for multimodal tasks**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2106.09889)
  <br> Lin Su, Nan Duan, Edward Cui, Lei Ji, **Chenfei Wu**, Huaishao Luo, Yongfei Liu, Ming Zhong, Taroon Bharti, Arun Sacheti.
  <br> Findings of ACL, 2021.

- **EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2310.08185)
  <br> Wang You, Wenshan Wu, Yaobo Liang, Shaoguang Mao, **Chenfei Wu**, Maosong Cao, Yuzhe Cai, Yiduo Guo, Yan Xia, Furu Wei, Nan Duan.
  <br> Arxiv 2023.

- **GameEval: Evaluating LLMs on Conversational Games**. [![](https://img.shields.io/badge/Paper-378CE7)](https://arxiv.org/abs/2402.10534)
  <br> Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, Nan Duan.
  <br> Arxiv 2023.
